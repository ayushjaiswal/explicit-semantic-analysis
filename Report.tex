\documentclass[conference]{IEEEtran}

\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption3}
\usepackage{array}
\DeclareGraphicsExtensions{.png,.JPG,.eps,.ps,.pdf,.eps.gz,.ps.gz,.eps.Z}
\usepackage{geometry}

\begin{document}

\title{A Domain-Independent Framework For Finding Semantic Relatedness Using Domain-Specific ESA}

\author{
    \IEEEauthorblockN{Ayush Jaiswal, Anunay Bhargava, Abdul Nazeer K. A.\\
    Dept. of Computer Science and Engineering\\National Institute of Technology Calicut}
}

% make the title area
\maketitle

\begin{abstract}
Explicit Semantic Analysis (ESA) with Wikipedia as its knowledge-base has been found to perform well for finding the semantic relatedness between text fragments. The method works by representing text fragments as weighted vectors of concepts derived from Wikipedia. Semantic relatedness between text fragments is then computed by using vector comparison metrics such as the cosine metric. Although, this method is found to perform very well for the task, and to correlate with human judgements, it is not applicable to the case of domain-specific specialized text fragments, such as those extracted from research papers, or complete documents pertaining to specific domains. We propose an extension of ESA which uses domain-specific knowledge-bases, such as MedlinePlus for text related to diseases, conditions, and wellness issues, and provides an improvement in the computation of semantic relatedness scores. We also incorporate the use of preprocessing tasks that have been modified to suit the needs of different domains. In this paper, we present a domain-independent framework in which the domain-specific knowledge-base and the specialized preprocessing tasks of a particular domain can be easily plugged in to find the semantic relatedness of text fragments in that domain.
\end{abstract}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Introduction}
In this section, we will discuss various terms and methods involved in finding the semantic relatedness of text fragments.

\subsection {Semantic Relatedness}

Semantic relatedness is the measure of how different text fragments are related to each other in their meanings. The problem of calculating semantic relatedness is an interesting and popular problem in the field of Natural Language Processing (NLP) and Information Retrieval (IR). The relatedness between the text fragments can't be calculated properly merely on the basis of the words, but also requires the understanding of the context of the sentence.

\subsection {Initial Work}

Early methods for calculating semantic relatedness were either statistical \cite{statistical, LSA}, which used no background knowledge, or lexical \cite{lexical}, which used very little world knowledge. Latent Semantic Analysis (LSA), used in previous work, \cite{LSA} learns word-concept relations without the use of human knowledge and is found to be very difficult in understanding, as the learnt concepts do not map to human concepts very well, or the mapping is very difficult to interpret.

\subsection {Explicit Semantic Analysis for Computing Semantic Relatedness}

Explicit Semantic Analysis (ESA) \cite{ESA} is a method to represent the meaning of text fragments that mimics the way human beings think about words, their meanings and the relationships between words and phrases. Human beings do not solely depend on the structure of words and sentences to understand their meaning or context. Every text fragment has some keywords that trigger some sort of understanding within the human mind. This understanding is based on the knowledge of one's environment and personal experiences. The keywords represent concepts that prevail deep in the human mind.

The ESA model can be used to find the semantic relatedness between text fragments by using explicit background knowledge such as articles from Wikipedia, representing human concepts. This gives a better structure for the representation of the semantics of text fragments, and allows finding the semantic relatedness by comparing text-semantics associations directly. Specifically, ESA converts text fragments to weighted vectors of concepts and calculates their semantic relatedness as the similarity of these vectors using metrics such as the cosine metric.

\subsection {Computing Domain-Specific Semantic Relatedness Using Domain-Specific ESA}

Although the ESA model using Wikipedia as knowledge-base performs well for general topics in calculating the semantic relatedness, it has subtle disadvantages \cite{insights}. One disadvantage is the high dimensionality of the concept space due to the copious number of articles present in Wikipedia, which introduces significant noise and distortion in the concept space. Another disadvantage that results from this is that specialized concepts either receive low association weights or are removed while filtering supposedly insignificant concepts. Moreover, using a broad knowledge-base means that the vocabulary in the articles representing concepts might not overlap with that of the domain-specific text fragments. ESA depends substantially on this vocabulary overlap and, hence, using a domain-specific knowledge-base makes more sense as there is a greater chance of such overlap \cite{insights}.

We propose an extension of the ESA model using domain-specific specialized knowledge-bases. The concept space, thus, has a dimensionality significantly less than that from Wikipedia, and has specialized documents included in the knowledge-base. This reduces the computational requirements of the model and works on domain-specific documents or text fragments of the respective domain with comparable results. We choose the field of diseases and health topics as our domain with concepts derived as different topics in MedlinePlus (www.nlm.nih.gov/medlineplus/â€Ž) dataset. Topics in the MedlinePlus dataset contain information only about diseases, conditions, and wellness issues.

Our framework for computing semantic relatedness of domain-specific text fragments that allows the knowledge-base and the domain-specific text preprocessing modules to be plugged into the framework thus making it applicable for any domain in general, by keeping the framework domain-independent.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Related work}

Gabrilovich and Markovitch proposed the ESA model for finding semantic relatedness of text fragments in their 2007 paper \cite{ESA}. The relatedness measure is calculated using prior background knowledge available to the model, Wikipedia in this case. The text fragments are converted into weighted vectors of concepts (Wikipedia articles) and their semantic relatedness is measured by comparing these vectors using metrics such as the cosine metric. Their method outperformed the earlier statistical~\cite{statistical}, lexical~\cite{lexical}, other Wikipedia-based methods \cite{WikiRelate}, and Latent Semantic Analysis~\cite{LSA} in finding semantic relatedness of text fragments, measured as correlation with human judgements. Gottron, Anderka and Stein \cite{insights} elucidates the working of ESA model, presents its shortcomings, and reports that a domain-specific knowledge-base is more suitable for the ESA model than Wikipedia.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

\section{Motivation}

The ESA model outperformed previously developed statistical, lexical, other Wikipedia-based methods, and Latent Semantic Analysis, but it is a very general approach towards finding semantic relatedness of text fragments and has some flaws~\cite{insights}. These are discussed as follows.

An obvious flaw in the Wikipedia-based model is that each word can map to various concepts across multiple domains. For example, ``cancer'' shows associations to the Wikipedia articles on the disease cancer, the constellation cancer, and the astrological sign cancer, to name a few. While the associations of the other words in the text fragment are expected to disambiguate the sense of cancer, it can be seen that the meaning of ``cancer'' is distorted by multiple mappings. Moreover, the large number of mappings is not needed when the text fragments are known to lie in a particular domain.

A major flaw in Wikipedia-based ESA is that the large number of articles in Wikipedia makes the development of the model time and space consuming as the weight for each concept has to be computed to find the final representation of text fragments as a weighted vector of concepts. The same problem arises when the semantic relatedness of two text fragments is computed. In terms of numbers, Wikipedia has 4.4 million English articles while MedlinePlus, which can be used as the knowledge-base for disease-related text fragments, has 953 topics, as in November, 2013.

The large number of articles also adds significant distortion to the concept space. The orthogonality of concepts, referred to as concept hypothesis, forms the basis of the ESA model. On close analysis, we found that domain-specific knowledge-bases are more orthogonal in concepts than Wikipedia. For example, the Wikipedia article on Diabetes Mellitus not only describes the same in general but also explains its different types. Along with that, separate Wikipedia articles are present for each type of Diabetes Mellitus. On the other hand, MedlinePlus has a topic for Diabetes Mellitus which explains it in general, and separate topics for its each type. We can see that the concepts in Wikipedia are thus more distorted. This leads us to question the claimed orthogonality of Wikipedia concepts. This problem is aggravated by the fact that Wikipedia is open for editing to everyone, and the articles can thus become more repetitive as people add more information to them. On the other hand, domain-specific knowledge-bases can be chosen from well-maintained sources, such as MedlinePlus for topics related to diseases, conditions, and wellness issues. Such knowledge-bases can be expected to be less repetitive in nature, and the concept space can then be expected to be less distorted.

Apart from the above flaws, the terms with low frequencies such as proper names, slangs, and technical terms are omitted during the process of Wikipedia-based ESA. Specialized Wikipedia articles are also removed from the knowledge-base, stating that they are too specific. By doing so, ESA becomes inapplicable to finding semantic relatedness of text fragments that are themselves specialized, such as those extracted from research papers or domain-specific articles. Another important point to note is that ESA relies heavily on the overlap of vocabulary in the text fragments being compared and the knowledge-base. Hence, it makes more sense to use domain-specific knowledge-bases to increase the chances of such overlap.

An added benefit of making our approach domain-specific is that we can incorporate text preprocessing methods, such as tokenization, lemmatization, etc. that have been developed specially for particular domains by using background knowledge of those domains. By making the entire semantic relatedness measurement framework domain-independent, these preprocessing tasks can be plugged in as modules along with the domain-specific knowledge-base.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Proposed Work}

We present a domain-independent framework for domain-specific semantic relatedness computation. The system allows the domain-specific parts, i.e., the knowledge-base and the preprocessing modules to be plugged in for the purpose. We begin with the domain of diseases, conditions, and wellness issues, using MedlinePlus as the knowledge-base.
Our method can be broken down into various components as follows.

\subsection{Preprocessor}

In this module, the data of the knowledge-base is tokenized, the tokens are normalized, and stop words are removed, using specialized techniques developed for the domain. The last step is to lemmatize the tokens by methods developed for the domain. These steps are crucial as they make the data consistent for use in the later components of our design. It should be noted that this component of the design is a domain-specific module which can be easily plugged into the framework by using the API of the framework and by following the specifications of the preprocessing module defined by the framework. This component is also used later to preprocess the text fragments for which semantic relatedness has to be calculated. It should be noted that other domain-specific preprocessing tasks, such as abbreviation resolution and coreference resolution, can also be added to this component.

\subsection{Inverted Index Generator}

This module creates the representation of each concept as weighted vector of words using the TFIDF scheme. The weighted inverted index of words $\rightarrow$ concepts (articles in the knowledge-base) is then generated to speed up the retrieval of the weights of various concepts for different words. These weights are used in later stages to generate the weighted vector of concepts that represent text fragments, and thus to calculate the semantic relatedness of text fragments. This component also saves this inverted index on a file so that it can be directly loaded for reuse. This saves the computational cost of creating the inverted index every time the system has to be used for computing semantic relatedness. This component of the framework is domain-independent.

\subsection{Initializer}

This component creates the platform for computing semantic relatedness of text fragments. It loads the already saved inverted index into memory, making it available, and allows it to be used multiple times to compute the semantic relatedness of various text fragment pairs. It can be seen that this component is domain-independent.

\subsection{Semantic Relatedness Calculator}

This component runs on the platform created by the Initializer component every time it is required to find the semantic relatedness of two text fragments. It accepts two text fragments as input and uses the preprocessor component to do initial transformations. These preprocessed text fragments are then converted to TFIDF vectors. To merge the vector of each text fragment with the inverted index for representing it as a weighted vector of concepts: for each concept, the concept-weight of each word in the text fragment is multiplied by its TFIDF value and summed up to get the weight of that concept for the text fragment. The resulting vector, consisting of the weight for each concept, is known as the interpretation vector. The semantic relatedness is then found between the two text fragments by computing the similarity of their respective interpretation vectors in the vector-space of concepts using the cosine metric. This component is also domain-independent.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

\section{Discussion and Results}

The design of the framework has been discussed in the previous section. We have implemented the entire framework in Python 2.7. We chose the domain of diseases, conditions, and wellness issues, and our knowledge-base as topics in MedlinePlus XML dump. Links for the topics and articles in MedlinePlus are available for download from http://www.nlm.nih.gov/medlineplus/xml.html in XML format. We parsed the XML file available on the MedlinePlus website, dated 14th September, 2013, and saved all the required data available on the links after cleaning HTML tags. We saved the data in XML format with root tags ``articles'' and ``topics'' for their respective categories, to suit our purpose.

To find out how well the semantic relatedness measures generated by our approach correlate with human judgements, we carried out a survey on text fragment pairs extracted from both MedlinePlus articles and Wikipedia articles. People participating in the survey were required to rate how well text-fragment pairs are related to each other on a scale of 1-10 (1 being ``totally unrelated'' and 10 being ``exactly same''). Correlation with human judegements has been used in earlier work on finding semantic relatedness of text as a measure of how well a particular approach works. The semantic relatedness scores generated by our system correlate with human judgements with r = 0.7-0.9 \textbf{needs to corrected}, while those generated by Wikipedia-based ESA correlated with human judgements with r = 0.3-0.7 \textbf{needs to be corrected}. Thus, the use of a domain-specific knowledge-base gives semantic relatedness scores that correlate better with human judgements. \textbf{write something about survey participants and no. of ratings}.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Conclusion}

We have presented a domain-independent framework for finding the semantic relatedness of domain-specific text fragments. The preprocessing tasks, specially developed for a domain, can be easily plugged into our framework, along with the knowledge-base of that domain. Using ESA with domain-specific knowledge-base to find the semantic relatedness of domain-specific text fragments solves the problems of concept orthogonality and dimensionality that are present in Wikipedia-based ESA. Moreover, it gives semantic relatedness scores that correlate better with human judgements as compared to Wikipedia-based ESA. Thus, we believe
\newpage
\noindent that our method is more applicable in the case of specialized text fragments such as those extracted from research papers or articles.

\begin{thebibliography}{1}
\bibitem{ESA}
E. Gabrilovich and S. Markovitch.
\newblock {\em Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis}.
\newblock Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pp. 1606-1611, 2007.
\bibitem{insights}
Thomas Gottron, Maik Anderka and Benno Stein.
\newblock {\em Insights into Explicit Semantic Analysis}.
\newblock Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.
\bibitem{statistical}
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
\newblock {\em Modern Information Retrieval}.
\newblock Addison Wesley, New York, NY, 1999.
\bibitem{lexical}
Alexander Budanitsky and Graeme Hirst.
\newblock {\em Evaluating wordnet-based measures of lexical semantic relatedness}.
\newblock Computational Linguistics, 32(1):13-47, 2006.
\bibitem{LSA}
S. Deerwester, S. Dumais, G. Furnas,T. Landauer, and R. Harshman.
\newblock {\em Indexing by Latent Semantic Analysis}.
\newblock JASIS, 41(6):391-407, 1990.
\bibitem{WikiRelate}
Michael Strube and Simone Paolo Ponzetto.
\newblock {\em WikiRelate! Computing Semantic Relatedness Using Wikipedia}.
\newblock In AAAIâ€™06, Boston, MA, 2006.
\end{thebibliography}

\end{document}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

% To include figure, copy and paste the code below

%\begin{figure} [hbtp]
%\begin{center}
%\includegraphics[scale=0.5]{core_inout_diagrm.JPG}
%\end{center}
%{\caption{Core I/O diagram}}
%\end{figure}
