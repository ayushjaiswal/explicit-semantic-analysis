\documentclass[journal,transmag]{IEEEtran}

\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{array}
\DeclareGraphicsExtensions{.png,.JPG,.eps,.ps,.pdf,.eps.gz,.ps.gz,.eps.Z}
\usepackage{geometry}

\begin{document}

\title{Explicit Semantic Analysis for Semantic Relatedness of Domain-Specific Text Fragments}

\author{
    \IEEEauthorblockN{Ayush Jaiswal, Anunay Bhargava}\\
   \emph{Guided by: Abdul Nazeer K A, Associate Professor, Dept. of CSE\\National Institute of Technology Calicut}
}

% make the title area
\maketitle

\begin{abstract}
Explicit Semantic Analysis (ESA) with Wikipedia as its knowledge-base has been found to perform well for finding the semantic relatedness between text fragments. The method works by representing text fragments as weighted vectors of concepts derived from Wikipedia. Semantic relatedness between text fragments is then computed by using vector comparison metrics such as the cosine metric. Although, this method is found to perform very well for the task, and to correlate with human judgements, it is not applicable in the case of domain-specific specialized text fragments, such as those extracted from research papers, or complete documents pertaining to specific domains. We propose an extension of ESA which uses domain-specific knowledge-bases, such as MedlinePlus for text related to diseases, conditions, and wellness issues, and provides an improvement in the computation of semantic relatedness of specialized text fragments. We also incorporate the use of preprocessing tasks that have been modified to suit the needs of different domains. Our aim is to develop a framework in which the domain-specific knowledge base and the specialized preprocessing tasks can be easily plugged in to find the semantic relatedness of text fragments in that domain.
\end{abstract}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{\textbf{Introduction}}
In this section, we will discuss the various terms and methods involved in finding the semantic relatedness of text fragments.

\subsection {\textbf{Semantic Relatedness}}

Semantic relatedness is the measure of how different text fragments are related to each other in their meanings. The problem of calculating semantic relatedness is an interesting and popular problem in the field of Natural Language Processing (NLP) and Information Retrieval (IR). The relatedness between the text fragments can’t be calculated properly merely on the basis of the words, but also requires the understanding of the context of the sentence.

\subsection {\textbf{Initial Work}}

Early methods for calculating semantic relatedness were either statistical \cite{statistical, LSA}, which used no background knowledge, or lexical \cite{lexical}, which used very little world knowledge. Latent Semantic Analysis (LSA) \cite{LSA} learns word-concept relations without the use of human knowledge and is found to be very difficult in understanding, as the learnt concepts do not map to human concepts very well, or the mapping is very difficult to interpret.

\subsection {\textbf{Explicit Semantic Analysis for Computing Semantic Relatedness}}

Explicit Semantic Analysis (ESA) \cite{ESA} is a method to represent the meaning of text fragments that mimics the way human beings think about words, their meanings and the relationships between words and phrases. Human beings do not solely depend on the structure of words and sentences to understand their meaning or context. Every text fragment has some keywords that trigger some sort of understanding within the human mind. This understanding is based on the knowledge of one's environment and personal experiences. The keywords represent concepts that prevail deep in the human mind.\\
The ESA model can be used to find the semantic relatedness between text fragments by using explicit background knowledge such as articles from Wikipedia, representing human concepts. This gives a better structure for the representation of the semantics of text fragments, and allows finding the semantic relatedness by comparing text-semantics associations directly. Specifically, ESA converts text fragments to weighted vectors of concepts and calculates their semantic relatedness by calculating the similarity of these vectors using metrics such as the cosine metric.

\subsection {\textbf{Computing Domain-Specific Semantic Relatedness Using Domain-Specific ESA}}

Although the ESA model using Wikipedia as knowledge-base performs well for general topics in calculating the semantic relatedness, it has subtle disadvantages \cite{insights}. One disadvantage is the high dimensionality of the concept space due to the copious number of articles present in Wikipedia, which introduces significant noise and distortion in the concept space. Another disadvantage that results from this is that specialized concepts either receive low association weights or are removed while filtering supposedly insignificant concepts. These disadvantages make this model inapplicable to finding the semantic relatedness of specialized text fragments or those that are very specific in nature, but contain words that occur in various domains. Moreover, using a broad knowledge-base means that the vocabulary in the articles representing concepts might not overlap with that of the domain-specific text fragments. ESA depends substantially on this vocabulary overlap and, hence, using a domain-specific knowledge-base makes more sense as there is a greater chance of such overlap.\\
We propose an extension of the ESA model using domain-specific specialized knowledge-bases. The concept space would thus have a dimensionality significantly less than that from Wikipedia, and will have specialized documents included in the knowledge-base. This will reduce the computational requirements of the model and is expected to work on specialized documents or text fragments of the respective domain with better results as it takes care of the aforementioned disadvantages of the Wikipedia-based ESA.  We choose the field of diseases and health topics as our domain with concepts derived as different topics in MedlinePlus (www.nlm.nih.gov/medlineplus/‎) dataset. Topics in the MedlinePlus dataset contain information only about diseases, conditions, and wellness issues.\\
Additionally, we aim to build a framework for computing semantic relatedness of domain-specific text fragments that allows the knowledge-base and the domain-specific text preprocessing modules to be plugged into the framework thus making it applicable for any domain in general, by keeping the framework domain-independent.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{\textbf{Related work}}

Gabrilovich and Markovitch proposed the ESA model for finding semantic relatedness in text fragments in their 2007 paper \cite{ESA}. The relatedness measure is calculated using prior background knowledge available to the model, Wikipedia in this case. The text fragments are converted into weighted vectors of concepts (Wikipedia articles) and their semantic relatedness is measured by comparing these vectors using metrics such as the cosine metric. There method outperformed the earlier statistical, lexical, other Wikipedia-based methods \cite{WikiRelate}, and Latent Semantic Analysis in finding semantic relatedness of text fragments, measured as correlation with human judgements.\\
Insights into Explicit Semantic Analysis by Gottron, Anderka and Stein (2011) \cite{insights} elucidates the working of ESA model and discusses its shortcomings. Firstly, the wide topic range in Wikipedia introduces noise and distortion due to the presence of homonyms. For example, ``cancer'' (constellation), ``cancer'' (astrology) and ``cancer'' (disease) all would are present in Wikipedia, along with other articles on “cancer”. Secondly, the similarity between the vectors is measured by their inner products, which means that the words occurring in the text fragments must also occur in the knowledge-base. Therefore, it is more sensible to use domain-specific knowledge-bases to compute the semantic relatedness of text fragments in that domain.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

\section{\textbf{Motivation}}
The ESA model outperformed previously developed statistical, lexical, other Wikipedia-based methods, and Latent Semantic Analysis, but it is a very general approach towards finding semantic relatedness of text fragments and has some flaws. These are discussed as follows.\\
An obvious flaw in the Wikipedia-based model is that each word can map to various concepts across multiple domains. For example, ``cancer'' shows associations to the Wikipedia articles on the disease cancer, the constellation cancer, and the astrological sign cancer, to name a few. While the associations of the other words in the text fragment are expected to disambiguate the sense of cancer, it can be seen that the meaning of ``cancer'' is distorted by multiple mappings. Moreover, the large number of mappings is not needed when the text fragments are known to lie in a particular domain.\\
A major flaw in Wikipedia-based ESA is that the large number of articles in Wikipedia makes the development of the model time and space consuming as the weight for each concept has to be computed to find the final representation of text fragments as a weighted vector of concepts. The same problem arises when the semantic relatedness of two text fragments is computed.\\
The large number of articles also adds significant distortion to the concept space. The orthogonality of concepts, referred to as concept hypothesis, forms the basis of the ESA model. On close analysis, we found that domain-specific knowledge-bases are more orthogonal in concepts than Wikipedia. For example, the Wikipedia article on Diabetes Mellitus not only describes the same in general but also explains its different types. Along with that, separate Wikipedia articles are present for each type of Diabetes Mellitus. On the other hand, MedlinePlus has a topic for Diabetes Mellitus which explains it in general, and separate topics for its each type. We can see that the concepts in Wikipedia are thus more distorted. This leads us to question the claimed orthogonality of Wikipedia concepts. This problem is aggravated by the fact that Wikipedia is open for editing to everyone, and the articles can thus become more repetitive as people add more information to them. On the other hand, domain-specific knowledge-bases can be chosen from well-maintained sources, such as MedlinePlus for topics related to diseases, conditions, and wellness issues. Such knowledge-bases can be expected to be less repetitive in nature, and the concept space can then be expected to be less distorted.\\
Apart from the above flaws, the terms with lesser frequencies such as proper names, slangs, and technical terms are omitted during the process. Specialized Wikipedia articles are also removed from the knowledge-base, stating that they are too specific. By doing so, ESA becomes inapplicable to finding semantic relatedness of text fragments that are themselves specialized, such as those extracted from research papers or domain-specific articles. Another important point to note is that ESA relies heavily on the overlap of vocabulary in the text fragments being compared and the knowledge-base. Hence, it makes more sense to use domain-specific knowledge-bases to increase the chances of such overlap.\\
An added benefit of making our approach domain-specific is that we can incorporate text preprocessing methods, such as tokenization, lemmatization, etc. that have been developed specially for particular domains by using background knowledge of those domains. By making the entire semantic relatedness measurement framework domain-independent, these preprocessing tasks can be plugged in as modules along with the domain-specific knowledge-base. Thus, the semantic relatedness of specialized text fragments can be computed.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{\textbf{Proposed Work}}
We develop a domain-independent framework for domain-specific semantic relatedness computation. The system allows the domain-specific parts, i.e., the knowledge-base and the preprocessing modules to be plugged in for the purpose. We begin with the domain of diseases, conditions, and wellness issues, using MedlinePlus as the knowledge-base.
Our method can be broken down into various components as follows.
\subsection{\textbf{Preprocessor}}
This module cleans the raw data from the corpus to remove HTML tags. The data is then tokenized, using techniques developed for the particular domain. The tokens are normalized and stop words are removed, again using specialized techniques. The last step is to lemmatize the tokens by methods developed for the domain. These steps are crucial as they make the data consistent for use in the later components of our design. It should be noted that this component of the design is a domain-specific module which can be easily plugged into the framework by using the API of the framework and by following the specifications of the preprocessing module defined by the framework. This component is also used later to preprocess the text fragments for which semantic relatedness has to be calculated.
\subsection{\textbf{Inverted Index Generator}}
This module creates the representation of each concept as weighted vector of words using the TFIDF scheme. The weighted inverted index of words $\rightarrow$ concepts (articles in the knowledge-base) is then generated to speed up the retrieval of the weights of various concepts for different words. These weights are used in later stages to generate the weighted vector of concepts that represent text fragments, and thus to calculate the semantic relatedness of text fragments. This component also saves this inverted index on a file so that it can be directly loaded for reuse. This saves the computational cost of creating the inverted index every time the system has to be used for computing semantic relatedness. This component of the framework is domain-independent.
\subsection{\textbf{Initializer}}
This component creates the platform for computing semantic relatedness of text fragments. It loads the already saved inverted index into memory, making it available, and allows it to be used multiple times to compute the semantic relatedness of various text fragment pairs. It can be seen that this component is domain-independent.
\subsection{\textbf{Semantic Relatedness Calculator}}
This component runs on the platform created by the Initializer component every time it is required to find the semantic relatedness of two text fragments. It accepts two text fragments as input and uses the preprocessor component to do initial transformations. These preprocessed text fragments are then converted to TFIDF vectors. To merge the vector of each text fragment with the inverted index for representing it as a weighted vector of concepts: for each concept, the concept-weight of each word in the text fragment is multiplied by its TFIDF value and summed up to get the weight of that concept for the text fragment. The resultant vector, consisting of the weight for each concept, is known as interpretation vector. The semantic relatedness is then found between the two text fragments by computing the similarity of their respective interpretation vectors in the vector-space of concepts using the cosine metric. This component is also domain-independent.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

\section{\textbf{Current Status}}
Our first step was to decide and design the various components of the framework to meet our goals. The final design has been discussed in the previous section. We have implemented the entire framework in Python 2.7.\\
We chose the domain of diseases, conditions, and wellness issues, and our knowledge-base as topics in MedlinePlus XML dump. Links for the topics and articles in MedlinePlus are available for download from http://www.nlm.nih.gov/medlineplus/xml.html in XML format. We parsed the XML file available on the MedlinePlus website, dated 14th September, 2013, and saved all the required data available on the links after cleaning HTML tags. We saved the data in XML format with root tags ``articles'' and ``topics'' for their respective categories, to suit our purpose.\\
We have been able to run our code on text-fragment pairs that we extracted from MedlinePlus articles. To find out how well the semantic relatedness measures generated by our approach correlate with human judgements, we are carrying out a survey. People participating in the survey are required to rate how well text-fragment pairs are related to each other on a scale of 1-10 (1 being ``totally unrelated'' and 10 being ``exactly same''). Once our survey is complete, we will find the correlation between the human judgements and the scores generated by our code. Correlation with human judegements has been used in earlier work on finding semantic relatedness of text as a measure of how well a particular approach works.
%-----------------------------------------------------------------------------------------------------------------------------------------------------------
\section{\textbf{Conclusion}}
We have developed a domain-independent framework for finding the semantic relatedness of domain-specific text fragments. The preprocessing tasks that have been specially developed for a particular domain can be easily plugged into our framework, along with the knowledge-base of that domain. Using ESA with domain-specific knowledge-base to find the semantic relatedness of domain-specific text fragments solves the problems of concept orthogonality and dimensionality that are present in Wikipedia-based ESA. Thus, we believe that our method is more applicable in the case of specialized text fragments such as those extracted from research papers or articles.

\newpage

\begin{thebibliography}{1}
\bibitem{ESA}
E. Gabrilovich and S. Markovitch.
\newblock {\em Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis}.
\newblock Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pp. 1606-1611, 2007.
\bibitem{insights}
Thomas Gottron, Maik Anderka and Benno Stein.
\newblock {\em Insights into Explicit Semantic Analysis}.
\newblock Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.
\bibitem{statistical}
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
\newblock {\em Modern Information Retrieval}.
\newblock Addison Wesley, New York, NY, 1999.
\bibitem{lexical}
Alexander Budanitsky and Graeme Hirst.
\newblock {\em Evaluating wordnet-based measures of lexical semantic relatedness}.
\newblock Computational Linguistics, 32(1):13–47, 2006.
\bibitem{LSA}
S. Deerwester, S. Dumais, G. Furnas,T. Landauer, and R. Harshman.
\newblock {\em Indexing by Latent Semantic Analysis}.
\newblock JASIS, 41(6):391–407, 1990.
\bibitem{WikiRelate}
Michael Strube and Simone Paolo Ponzetto.
\newblock {\em WikiRelate! Computing Semantic Relatedness Using Wikipedia}.
\newblock In AAAI’06, Boston, MA, 2006.
\end{thebibliography}

\end{document}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------

% To include figure, copy and paste the code below

%\begin{figure} [hbtp]
%\begin{center}
%\includegraphics[scale=0.5]{core_inout_diagrm.JPG}
%\end{center}
%{\caption{Core I/O diagram}}
%\end{figure}
